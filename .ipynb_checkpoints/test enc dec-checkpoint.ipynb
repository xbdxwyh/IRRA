{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c9945f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61235d36",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model.attn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbartmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BartForLM\n",
      "File \u001b[1;32mE:\\Share\\jupyterDir\\IRRA\\model\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuild\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_model\n",
      "File \u001b[1;32mE:\\Share\\jupyterDir\\IRRA\\model\\build.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrderedDict\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config,SelfAttention,OutputLayer\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIRRA\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, args, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m11003\u001b[39m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'model.attn'"
     ]
    }
   ],
   "source": [
    "from model.bartmodel import BartForLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddf14dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dfe24bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = BartConfig(\n",
    "    encoder_layers=1,\n",
    "    decoder_layers=1,\n",
    "    d_model=768\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "592ae41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BartForLM(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "739c6a48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForLM(\n",
       "  (model): BartDecoderWrapper(\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f446ddc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb3678b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_weights(model: nn.Module):\n",
    "    \"\"\"Convert applicable model parameters to fp16\"\"\"\n",
    "\n",
    "    def _convert_weights_to_fp16(l):\n",
    "        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):\n",
    "            l.weight.data = l.weight.data.half()\n",
    "            if l.bias is not None:\n",
    "                l.bias.data = l.bias.data.half()\n",
    "\n",
    "        if isinstance(l, nn.MultiheadAttention):\n",
    "            for attr in [*[f\"{s}_proj_weight\" for s in [\"in\", \"q\", \"k\", \"v\"]], \"in_proj_bias\", \"bias_k\", \"bias_v\"]:\n",
    "                tensor = getattr(l, attr)\n",
    "                if tensor is not None:\n",
    "                    tensor.data = tensor.data.half()\n",
    "\n",
    "        for name in [\"text_projection\", \"proj\", \"mcq_proj\", \"prefix\"]:\n",
    "            if hasattr(l, name):\n",
    "                attr = getattr(l, name)\n",
    "                if attr is not None:\n",
    "                    attr.data = attr.data.half()\n",
    "\n",
    "    model.apply(_convert_weights_to_fp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fde632b",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d99c7f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn((8,13,768)).cuda()\n",
    "outputs = torch.randn((8,17,768)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47f9a94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to = model(\n",
    "#     inputs_embeds = inputs.to(torch.float16),\n",
    "#     decoder_inputs_embeds = outputs.to(torch.float16)\n",
    "# )\n",
    "to = model(\n",
    "    inputs_embeds = inputs.to(torch.float16),\n",
    "    #is_casual=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61959df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 13, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "beb23fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to.last_hidden_state.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b5e21cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total params: 12M'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Total params: %2.fM' % (sum(p.numel() for p in model.parameters()) / 1000000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3dad8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
